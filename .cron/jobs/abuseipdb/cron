#!/usr/bin/env bash

set -e

# Setup
cd "$(dirname $0)"

GIT_ROOT=$(git rev-parse --show-toplevel)
DB_PATH=$GIT_ROOT/db
mkdir -p $DB_PATH
TEMPFILE=$(mktemp)
TEMPDIR=$(mktemp -d)

cd $TEMPDIR

# Debug
echo "Public IP:"
echo $(timeout 2s curl --no-progress-meter ipv4.icanhazip.com)
echo

echo 'âœ” Debug...'
date '+%Y/%m/%d %H:%M:%S'
bkt --ttl=6h -- date '+%Y/%m/%d %H:%M:%S'
echo


echo 'âœ” Download abuseipdb...'
# Use a TTL of 2.5 hours (~ 9/10 requests if verified webmaster)
bkt --ttl=180min -- curl https://api.abuseipdb.com/api/v2/blacklist \
  --get \
  --max-time 10 \
  --user-agent "" \
  --no-progress-meter \
  -d confidenceMinimum=100 \
  -d limit=9999999 \
  -H "Key: $ABUSEIPDB_TOKEN" \
  -H "Accept: text/plain" \
  --fail \
  -w "\n" \
  -o TEMPFILE.1 || true

echo 'âœ” Download & decorate with extra sources ...'
echo '#2'
curl -sL https://abuseipdb.tmiland.com/abuseipdb.txt \
  --compressed --max-time 10 -G -sL --fail -o TEMPFILE.2 || true

echo '#3'
curl -sL https://raw.githubusercontent.com/LittleJake/ip-blacklist/main/abuseipdb_blacklist_ip_score_100.txt \
  --compressed --max-time 10 -G -sL --fail -o TEMPFILE.3 || true

# ðŸ’© Whitelisted scanner: Palo Alto
echo '#4'
curl -sL https://raw.githubusercontent.com/borestad/iplists/refs/heads/main/paloaltonetworks/paloaltonetworks.ipv4 | \
  grepip | xargs -n1 $GIT_ROOT/.cron/scripts/abuseipdb-check | jq '[.[] | select(.numReports > 1)]' | grepip >| TEMPFILE.4 || true

# ðŸ’© Whitelisted scanner: Censys
echo '#5'
curl -sL https://raw.githubusercontent.com/borestad/iplists/refs/heads/main/censys/censys.ipv4 | \
  grepip | xargs -n1 $GIT_ROOT/.cron/scripts/abuseipdb-check | jq '[.[] | select(.numReports > 1)]' | grepip >| TEMPFILE.5 || true

# ðŸ’© Whitelisted scanner: Project Sonar (Rapid7)
echo '#6'
curl -sL https://raw.githubusercontent.com/borestad/iplists/refs/heads/main/project-sonar/project-sonar.ipv4 | \
  grepip | xargs -n1 $GIT_ROOT/.cron/scripts/abuseipdb-check | jq '[.[] | select(.numReports > 1)]' | grepip >| TEMPFILE.6 || true

# Redundancy:
# - Separate private cache (1 of 5 requests / day) to avoid breaking the 5 free run limit / day
# - If above urls fail due to github actions being flaky, still have somewhat fresh data.
# echo 'âœ” Download from cache'
# curl "$CRONSRC_URL" -H "$CRONSRC_HEADER" --compressed --max-time 10 -G -sL -w "\n\n" --fail -o TEMPFILE.4 || true

# echo 'âœ” Stats'
# for FILE in TEMPFILE.*; do printf "$FILE "; wc -l < $FILE; done

echo 'âœ” Squash all sources (by design: fail if no sources worked)'
grep -h "" TEMPFILE.* >> $TEMPFILE

echo 'âœ” Validate: Clean comments'
cat $TEMPFILE | shfmt -mn | sponge $TEMPFILE

echo 'âœ” Validate: Extract ipv6 data'
  grep ':' $TEMPFILE | sort | tac | cidr-merger | sponge $TEMPFILE.ipv6

echo 'âœ” Validate: Extract ipv4 data'
  grep -v ":" $TEMPFILE | \
  iprange --print-single-ips \
  > $TEMPFILE.ipv4

# 3. Validate data
LINES=`wc -l < $TEMPFILE.ipv4`
if [[ "$LINES" -gt "1000" ]]; then
  echo "âœ” Validate: File contains: $LINES lines"
  mv $TEMPFILE.ipv4 $DB_PATH/abuseipdb-s100-latest.ipv4
  mv $TEMPFILE.ipv6 $DB_PATH/abuseipdb-s100-latest.ipv6
else
  echo "âŒ Validation failed"
  echo
  echo "-----------------------------------------------------"
  cat $TEMPFILE
  echo "-----------------------------------------------------"
  cat $TEMPFILE.ipv4
  echo "-----------------------------------------------------"
  exit 1
fi

echo
echo 'âœ” Aggregate: Create folders'
DATE=$(date +%F)
DATE_DIR=$DB_PATH/$DATE
mkdir -pv $DATE_DIR && cd $DATE_DIR

echo 'âœ” Aggregate: Copy latest to correct date folder'
cp $DB_PATH/abuseipdb-s100-latest.ipv4 "$DATE_DIR/tmp-$(date +%H-%m-%S).ipv4"
cp $DB_PATH/abuseipdb-s100-latest.ipv6 "$DATE_DIR/tmp-$(date +%H-%m-%S).ipv6"

echo 'âœ” Aggregate: Squash ipv4 data'
iprange --print-single-ips *.ipv4 | sponge $(date +%Y-%m-%d).ipv4

echo 'âœ” Aggregate: Squash ipv6 data'
cat *.ipv6 | grep ':' | sort | uniq | sort | sponge $(date +%Y-%m-%d).ipv6

echo
echo 'âœ” Cleanup: Remove temp files'
rm -f tmp*.ipv4
rm -f tmp*.ipv6
